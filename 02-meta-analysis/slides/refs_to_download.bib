@ARTICLE{Shi2019-pj,
  title = {The trim-and-fill method for publication bias: practical guidelines
  and recommendations based on a large database of meta-analyses: practical
  guidelines and recommendations based on a large database of meta-analyses},
  author = {Shi, Linyu and Lin, Lifeng},
  journaltitle = {Medicine (Baltimore)},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  volume = {98},
  issue = {23},
  pages = {e15987},
  date = {2019-06},
  doi = {10.1097/MD.0000000000015987},
  pmc = {PMC6571372},
  pmid = {31169736},
  issn = {0025-7974,1536-5964},
  abstract = {Publication bias is a type of systematic error when synthesizing
  evidence that cannot represent the underlying truth. Clinical studies with
  favorable results are more likely published and thus exaggerate the
  synthesized evidence in meta-analyses. The trim-and-fill method is a popular
  tool to detect and adjust for publication bias. Simulation studies have been
  performed to assess this method, but they may not fully represent realistic
  settings about publication bias. Based on real-world meta-analyses, this
  article provides practical guidelines and recommendations for using the
  trim-and-fill method. We used a worked illustrative example to demonstrate the
  idea of the trim-and-fill method, and we reviewed three estimators (R0, L0,
  and Q0) for imputing missing studies. A resampling method was proposed to
  calculate P values for all 3 estimators. We also summarized available
  meta-analysis software programs for implementing the trim-and-fill method.
  Moreover, we applied the method to 29,932 meta-analyses from the Cochrane
  Database of Systematic Reviews, and empirically evaluated its overall
  performance. We carefully explored potential issues occurred in our analysis.
  The estimators L0 and Q0 detected at least one missing study in more
  meta-analyses than R0, while Q0 often imputed more missing studies than L0.
  After adding imputed missing studies, the significance of heterogeneity and
  overall effect sizes changed in many meta-analyses. All estimators generally
  converged fast. However, L0 and Q0 failed to converge in a few meta-analyses
  that contained studies with identical effect sizes. Also, P values produced by
  different estimators could yield different conclusions of publication bias
  significance. Outliers and the pre-specified direction of missing studies
  could have influential impact on the trim-and-fill results. Meta-analysts are
  recommended to perform the trim-and-fill method with great caution when using
  meta-analysis software programs. Some default settings (e.g., the choice of
  estimators and the direction of missing studies) in the programs may not be
  optimal for a certain meta-analysis; they should be determined on a
  case-by-case basis. Sensitivity analyses are encouraged to examine effects of
  different estimators and outlying studies. Also, the trim-and-fill estimator
  should be routinely reported in meta-analyses, because the results depend
  highly on it.},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6571372/},
  language = {en}
}

@ARTICLE{Veroniki2016-zs,
  title = {Methods to estimate the between-study variance and its uncertainty in
  meta-analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang
  and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and
  Higgins, Julian P T and Langan, Dean and Salanti, Georgia},
  journaltitle = {Res. Synth. Methods},
  publisher = {Wiley},
  volume = {7},
  issue = {1},
  pages = {55-79},
  date = {2016-03},
  doi = {10.1002/jrsm.1164},
  pmc = {PMC4950030},
  pmid = {26332144},
  issn = {1759-2879,1759-2887},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of
  an outcome of interest. However, inference about between-study variability,
  which is typically modelled using a between-study variance parameter, is
  usually an additional aim. The DerSimonian and Laird method, currently widely
  used by default to estimate the between-study variance, has been long
  challenged. Our aim is to identify known methods for estimation of the
  between-study variance and its corresponding uncertainty, and to summarise the
  simulation and empirical evidence that compares them. We identified 16
  estimators for the between-study variance, seven methods to calculate
  confidence intervals, and several comparative studies. Simulation studies
  suggest that for both dichotomous and continuous data the estimator proposed
  by Paule and Mandel and for continuous data the restricted maximum likelihood
  estimator are better alternatives to estimate the between-study variance.
  Based on the scenarios and results presented in the published studies, we
  recommend the Q-profile method and the alternative approach based on a
  'generalised Cochran between-study variance statistic' to compute
  corresponding confidence intervals around the resulting estimates. Our
  recommendations are based on a qualitative evaluation of the existing
  literature and expert consensus. Evidence-based recommendations require an
  extensive simulation study where all methods would be compared under the same
  scenarios.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:Xc-mKOjpdrwC},
  keywords = {bias; confidence interval; coverage probability; heterogeneity;
  mean squared error},
  language = {en}
}

@ARTICLE{Rosenthal1979-yx,
  title = {The file drawer problem and tolerance for null results},
  author = {Rosenthal, Robert},
  journaltitle = {Psychol. Bull.},
  publisher = {American Psychological Association (APA)},
  volume = {86},
  issue = {3},
  pages = {638-641},
  date = {1979-05},
  doi = {10.1037/0033-2909.86.3.638},
  issn = {0033-2909,1939-1455},
  url = {https://psycnet.apa.org/record/1979-27602-001},
  language = {en}
}

@ARTICLE{Orwin1983-vu,
  title = {A fail-SafeN for effect size in meta-analysis},
  author = {Orwin, Robert G},
  journaltitle = {J. Educ. Stat.},
  publisher = {American Educational Research Association (AERA)},
  volume = {8},
  issue = {2},
  pages = {157-159},
  date = {1983-06},
  doi = {10.3102/10769986008002157},
  issn = {0362-9791,2328-0735},
  abstract = {Rosenthan’s (1979) concept of fail-safe N has thus far been
  applied to probability levels exclusively. This note introduces a fail-safe N
  for effect size.},
  url = {https://journals.sagepub.com/doi/abs/10.3102/10769986008002157},
  language = {en}
}

@ARTICLE{Rosenberg2005-ie,
  title = {The file-drawer problem revisited: a general weighted method for
  calculating fail-safe numbers in meta-analysis},
  author = {Rosenberg, Michael S},
  journaltitle = {Evolution},
  publisher = {Wiley},
  volume = {59},
  issue = {2},
  pages = {464-468},
  date = {2005-02},
  doi = {10.1111/j.0014-3820.2005.tb01004.x},
  pmid = {15807430},
  issn = {0014-3820,1558-5646},
  abstract = {Quantitative literature reviews such as meta-analysis are becoming
  common in evolutionary biology but may be strongly affected by publication
  biases. Using fail-safe numbers is a quick way to estimate whether publication
  bias is likely to be a problem for a specific study. However, previously
  suggested fail-safe calculations are unweighted and are not based on the
  framework in which most meta-analyses are performed. A general, weighted
  fail-safe calculation, grounded in the meta-analysis framework, applicable to
  both fixed- and random-effects models, is proposed. Recent meta-analyses
  published in Evolution are used for illustration.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0014-3820.2005.tb01004.x},
  language = {en}
}

@ARTICLE{Citkowicz2017-ox,
  title = {A parsimonious weight function for modeling publication bias},
  author = {Citkowicz, Martyna and Vevea, Jack L},
  journaltitle = {Psychol. Methods},
  publisher = {psycnet.apa.org},
  volume = {22},
  issue = {1},
  pages = {28-41},
  date = {2017-03},
  doi = {10.1037/met0000119},
  pmid = {28252998},
  issn = {1082-989X,1939-1463},
  abstract = {Quantitative research literature is often biased because studies
  that fail to find a significant effect (or that demonstrate effects in an
  undesired or unexpected direction) are less likely to be published. This
  phenomenon, termed publication bias, can cause problems when researchers
  attempt to synthesize results using meta-analytic methods. Various techniques
  exist that attempt to estimate and correct meta-analyses for publication bias.
  However, there is no single method that can (a) account for continuous
  moderators by including them within the model, (b) allow for substantial data
  heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal
  test for publication bias, and (e) allow for correction when only a small
  number of effects is included in the analysis. This article describes a method
  that we believe helps fill that gap. The model uses the beta density as a
  weight function that represents the selection process and provides adjusted
  parameter estimates that account for publication bias. Use of the beta density
  allows us to represent selection using fewer parameters than similar models so
  that the proposed model is suitable for meta-analyses that include relatively
  few studies. We explain the model and its rationale, illustrate its use with a
  real data set, and describe the results of a simulation study that shows the
  model's utility. (PsycINFO Database Record},
  url = {https://psycnet.apa.org/journals/met/22/1/28/},
  language = {en}
}

@ARTICLE{McShane2016-bk,
  title = {Adjusting for publication bias in meta-analysis: An evaluation of
  selection methods and some cautionary notes: An evaluation of selection
  methods and some cautionary notes},
  author = {McShane, Blakeley B and Böckenholt, Ulf and Hansen, Karsten T},
  journaltitle = {Perspect. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {5},
  pages = {730-749},
  date = {2016-09},
  doi = {10.1177/1745691616662243},
  pmid = {27694467},
  issn = {1745-6916,1745-6924},
  abstract = {We review and evaluate selection methods, a prominent class of
  techniques first proposed by Hedges (1984) that assess and adjust for
  publication bias in meta-analysis, via an extensive simulation study. Our
  simulation covers both restrictive settings as well as more realistic settings
  and proceeds across multiple metrics that assess different aspects of model
  performance. This evaluation is timely in light of two recently proposed
  approaches, the so-called p-curve and p-uniform approaches, that can be viewed
  as alternative implementations of the original Hedges selection method
  approach. We find that the p-curve and p-uniform approaches perform reasonably
  well but not as well as the original Hedges approach in the restrictive
  setting for which all three were designed. We also find they perform poorly in
  more realistic settings, whereas variants of the Hedges approach perform well.
  We conclude by urging caution in the application of selection methods: Given
  the idealistic model assumptions underlying selection methods and the
  sensitivity of population average effect size estimates to them, we advocate
  that selection methods should be used less for obtaining a single estimate
  that purports to adjust for publication bias ex post and more for sensitivity
  analysis-that is, exploring the range of estimates that result from assuming
  different forms of and severity of publication bias.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1745691616662243},
  keywords = {effect size; meta-analysis; p-curve; p-uniform; selection methods},
  language = {en}
}

@ARTICLE{Bartos2022-im,
  title = {Adjusting for publication bias in JASP and R: Selection models,
  PET-PEESE, and robust Bayesian meta-analysis},
  author = {Bartoš, František and Maier, Maximilian and Quintana, Daniel S and
  Wagenmakers, Eric-Jan},
  journaltitle = {Adv. Methods Pract. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {5},
  issue = {3},
  pages = {251524592211092},
  date = {2022-07},
  doi = {10.1177/25152459221109259},
  issn = {2515-2459,2515-2467},
  abstract = {Meta-analyses are essential for cumulative science, but their
  validity can be compromised by publication bias. To mitigate the impact of
  publication bias, one may apply publication-bias-adjustment techniques such as
  precision-effect test and precision-effect estimate with standard errors
  (PET-PEESE) and selection models. These methods, implemented in JASP and R,
  allow researchers without programming experience to conduct state-of-the-art
  publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how
  to conduct a publication-bias-adjusted meta-analysis in JASP and R and
  interpret the results. First, we explain two frequentist bias-correction
  methods: PET-PEESE and selection models. Second, we introduce robust Bayesian
  meta-analysis, a Bayesian approach that simultaneously considers both
  PET-PEESE and selection models. We illustrate the methodology on an example
  data set, provide an instructional video ( https://bit.ly/pubbias ) and an
  R-markdown script ( https://osf.io/uhaew/ ), and discuss the interpretation of
  the results. Finally, we include concrete guidance on reporting the
  meta-analytic results in an academic article.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/25152459221109259},
  language = {en}
}

@ARTICLE{Maier2023-js,
  title = {Robust Bayesian meta-analysis: Addressing publication bias with
  model-averaging},
  author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
  journaltitle = {Psychol. Methods},
  volume = {28},
  issue = {1},
  pages = {107-122},
  date = {2023-02},
  doi = {10.1037/met0000405},
  pmid = {35588075},
  issn = {1082-989X,1939-1463},
  abstract = {Meta-analysis is an important quantitative tool for cumulative
  science, but its application is frustrated by publication bias. In order to
  test and adjust for publication bias, we extend model-averaged Bayesian
  meta-analysis with selection models. The resulting robust Bayesian
  meta-analysis (RoBMA) methodology does not require all-or-none decisions about
  the presence of publication bias, can quantify evidence in favor of the
  absence of publication bias, and performs well under high heterogeneity. By
  model-averaging over a set of 12 models, RoBMA is relatively robust to model
  misspecification and simulations show that it outperforms existing methods. We
  demonstrate that RoBMA finds evidence for the absence of publication bias in
  Registered Replication Reports and reliably avoids false positives. We provide
  an implementation in R so that researchers can easily use the new methodology
  in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  url = {https://psycnet.apa.org/record/2022-62552-001},
  language = {en}
}

@MISC{Viechtbauer2021-od,
  title = {Selection models for publication bias in meta-analysis. Presentation
  at ESMARConf2021},
  author = {Viechtbauer, Wolfgang},
  publisher = {figshare},
  date = {2021-01-25},
  doi = {10.6084/M9.FIGSHARE.13637900.V1},
  abstract = {The non-replicability of certain findings in various disciplines
  has brought further attention to the problem that the published literature -
  which predominantly forms the evidence basis of research syntheses - may not
  be representative of all research that has been conducted on a particular
  topic. More specifically, concerns have been raised for a long time that
  statistically significant findings are overrepresented in the published
  literature, a phenomenon usually referred to as publication bias, which in
  turn can lead to biased conclusions. Various methods have been proposed in the
  meta-analytic literature for detecting the presence of publication bias,
  estimating its potential impact, and correcting for it. So-called selection
  models are among the most sophisticated methods for this purpose, as they
  attempt to directly model the selection process. If a particular selection
  model is an adequate approximation for the underlying selection process, then
  the model provides estimates of the parameters of interest (e.g., the average
  true effect and the amount of heterogeneity in the true effects) that are
  'corrected' for this selection process (i.e., they are estimates of the
  parameters in the population of studies before any selection has taken place).
  In this talk, I will briefly describe a variety of models for this purpose and
  illustrate their application with the metafor package in R.},
  url = {https://figshare.com/articles/conference_contribution/Selection_models_for_publication_bias_in_meta-analysis_Presentation_at_ESMARConf2021/13637900}
}

@ARTICLE{Oliveira2023-yf,
  title = {Internet-delivered cognitive behavioral therapy for anxiety among
  university students: A systematic review and meta-analysis},
  author = {Oliveira, Cláudia and Pacheco, Mara and Borges, Janete and Meira,
  Liliana and Santos, Anita},
  journaltitle = {Internet Interv.},
  publisher = {Elsevier BV},
  volume = {31},
  issue = {100609},
  pages = {100609},
  date = {2023-03-01},
  doi = {10.1016/j.invent.2023.100609},
  pmc = {PMC9982642},
  pmid = {36873307},
  issn = {2214-7829},
  abstract = {University years are marked by multiple stressors. Consequently,
  university students often report anxiety symptoms or disorders, but most
  remain untreated. Internet-delivered cognitive behavioral therapy (ICBT) has
  been proposed as an alternative to address known help-seeking barriers, which
  were aggravated during the COVID-19 pandemic. This meta-analysis aims to
  evaluate the efficacy of ICBT for university students with anxiety. A
  systematic search on three databases, EBSCOhost, PubMed, and Web of Science,
  and a manual search were performed. Fifteen studies were identified, including
  a total of 1619 participants. Seven studies evaluated ICBT treatment for both
  anxiety and depression, three for social anxiety, two for generalized anxiety,
  while the remaining (k = 3) only targeted anxiety, test anxiety, and
  comorbidity between anxiety and insomnia. Analyses were performed based on a
  random-effects model using the metafor package in R. The results indicated
  that ICBT had a significant and positive effect on university students with
  anxiety compared to controls at post-test (g = -0.48; 95 \% CI: -0.63, -0.27;
  p < .001, I 2 = 67.30 \%). Nevertheless, more research is required to
  determine the intervention components that are more relevant for therapeutic
  change, how much guidance is required to produce better outcomes, and how
  patient engagement can be improved.},
  url = {http://dx.doi.org/10.1016/j.invent.2023.100609},
  urldate = {2023-09-05},
  keywords = {Anxiety; ICBT; Internet interventions; Internet-delivered
  cognitive behavioral therapy; University students},
  language = {en}
}

@ARTICLE{Dear2019-qc,
  title = {Do ‘watching eyes’ influence antisocial behavior? A systematic review
  \& meta-analysis},
  author = {Dear, Keith and Dutton, Kevin and Fox, Elaine},
  journaltitle = {Evol. Hum. Behav.},
  publisher = {Elsevier BV},
  volume = {40},
  issue = {3},
  pages = {269-280},
  date = {2019-05-01},
  doi = {10.1016/j.evolhumbehav.2019.01.006},
  issn = {1090-5138,1879-0607},
  abstract = {Eye cues have been shown to stimulate rapid, reflexive,
  unconscious processing and in many experimental settings to cue increased
  prosocial and decrea…},
  url = {http://dx.doi.org/10.1016/j.evolhumbehav.2019.01.006},
  urldate = {2023-09-05},
  language = {en}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

@ARTICLE{Duval2000-ym,
  title = {Trim and fill: A simple funnel-plot-based method of testing and
  adjusting for publication bias in meta-analysis},
  author = {Duval, S and Tweedie, R},
  journaltitle = {Biometrics},
  publisher = {Wiley},
  volume = {56},
  issue = {2},
  pages = {455-463},
  date = {2000-06},
  doi = {10.1111/j.0006-341x.2000.00455.x},
  pmid = {10877304},
  issn = {0006-341X,1541-0420},
  abstract = {We study recently developed nonparametric methods for estimating
  the number of missing studies that might exist in a meta-analysis and the
  effect that these studies might have had on its outcome. These are simple
  rank-based data augmentation techniques, which formalize the use of funnel
  plots. We show that they provide effective and relatively powerful tests for
  evaluating the existence of such publication bias. After adjusting for missing
  studies, we find that the point estimate of the overall effect size is
  approximately correct and coverage of the effect size confidence intervals is
  substantially improved, in many cases recovering the nominal confidence levels
  entirely. We illustrate the trim and fill method on existing meta-analyses of
  studies in clinical trials and psychometrics.},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2000.00455.x},
  keywords = {Meta-analysis},
  language = {en}
}

@MISC{Viechtbauer2010-xz,
  title = {Conducting meta-analyses in {R} with the {metafor} package},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {Journal of Statistical Software},
  volume = {36},
  issue = {3},
  pages = {1-48},
  date = {2010},
  doi = {10.18637/jss.v036.i03},
  url = {https://doi.org/10.18637/jss.v036.i03},
  keywords = {Meta-analysis}
}

@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386},
  keywords = {MA Unconscious WM}
}

@ARTICLE{Higgins2002-fh,
  title = {Quantifying heterogeneity in a meta-analysis},
  author = {Higgins, Julian P T and Thompson, Simon G},
  journaltitle = {Stat. Med.},
  volume = {21},
  issue = {11},
  pages = {1539-1558},
  date = {2002-06-15},
  doi = {10.1002/sim.1186},
  pmid = {12111919},
  issn = {0277-6715},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines
  the difficulty in drawing overall conclusions. This extent may be measured by
  estimating a between-study variance, but interpretation is then specific to a
  particular treatment effect metric. A test for the existence of heterogeneity
  exists, but depends on the number of studies in the meta-analysis. We develop
  measures of the impact of heterogeneity on a meta-analysis, from mathematical
  criteria, that are independent of the number of studies and the treatment
  effect metric. We derive and propose three suitable statistics: H is the
  square root of the chi2 heterogeneity statistic divided by its degrees of
  freedom; R is the ratio of the standard error of the underlying mean from a
  random effects meta-analysis to the standard error of a fixed effect
  meta-analytic estimate, and I2 is a transformation of (H) that describes the
  proportion of total variation in study estimates that is due to heterogeneity.
  We discuss interpretation, interval estimates and other properties of these
  measures and examine them in five example data sets showing different amounts
  of heterogeneity. We conclude that H and I2, which can usually be calculated
  for published meta-analyses, are particularly useful summaries of the impact
  of heterogeneity. One or both should be presented in published meta-analyses
  in preference to the test for heterogeneity.},
  url = {http://dx.doi.org/10.1002/sim.1186},
  language = {en}
}

@ARTICLE{Viechtbauer2005-zt,
  title = {Bias and efficiency of meta-analytic variance estimators in the
  random-effects model},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {J. Educ. Behav. Stat.},
  publisher = {American Educational Research Association (AERA)},
  volume = {30},
  issue = {3},
  pages = {261-293},
  date = {2005-09},
  doi = {10.3102/10769986030003261},
  issn = {1076-9986,1935-1054},
  abstract = {The meta-analytic random effects model assumes that the
  variability in effect size estimates drawn from a set of studies can be
  decomposed into two parts: heterogeneity due to random population effects and
  sampling variance. In this context, the usual goal is to estimate the central
  tendency and the amount of heterogeneity in the population effect sizes. The
  amount of heterogeneity in a set of effect sizes has implications regarding
  the interpretation of the meta-analytic findings and often serves as an
  indicator for the presence of potential moderator variables. Five population
  heterogeneity estimators were compared in this article analytically and via
  Monte Carlo simulations with respect to their bias and efficiency.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:qjMakFHDy7sC},
  language = {en}
}

@ARTICLE{Hedges2021-of,
  title = {The Design of Replication Studies},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {J. R. Stat. Soc. Ser. A Stat. Soc.},
  publisher = {Oxford Academic},
  volume = {184},
  issue = {3},
  pages = {868-886},
  date = {2021-03-31},
  doi = {10.1111/rssa.12688},
  issn = {0964-1998,1467-985X},
  abstract = {Abstract. Empirical evaluations of replication have become
  increasingly common, but there has been no unified approach to doing so. Some
  evaluations conduct onl},
  url = {https://academic.oup.com/jrsssa/article/184/3/868/7068411},
  urldate = {2023-09-01},
  keywords = {replication-methods;replication-summer-school-2023},
  language = {en}
}

@ARTICLE{Guan2016-kn,
  title = {A Bayesian approach to mitigation of publication bias},
  author = {Guan, Maime and Vandekerckhove, Joachim},
  journaltitle = {Psychon. Bull. Rev.},
  volume = {23},
  issue = {1},
  pages = {74-86},
  date = {2016-02},
  doi = {10.3758/s13423-015-0868-6},
  pmid = {26126776},
  issn = {1069-9384,1531-5320},
  abstract = {The reliability of published research findings in psychology has
  been a topic of rising concern. Publication bias, or treating positive
  findings differently from negative findings, is a contributing factor to this
  "crisis of confidence," in that it likely inflates the number of
  false-positive effects in the literature. We demonstrate a Bayesian model
  averaging approach that takes into account the possibility of publication bias
  and allows for a better estimate of true underlying effect size. Accounting
  for the possibility of bias leads to a more conservative interpretation of
  published studies as well as meta-analyses. We provide mathematical details of
  the method and examples.},
  url = {http://dx.doi.org/10.3758/s13423-015-0868-6},
  keywords = {Bayesian inference and parameter estimation; Bayesian statistics;
  Math modeling and model selection; Meta-analysis;Bayesian Statistics},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychol. Methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replication-methods},
  language = {en}
}

